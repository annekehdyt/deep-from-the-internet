{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM by Example\n",
    "\n",
    "https://towardsdatascience.com/lstm-by-example-using-tensorflow-feb0c1968537 <br> <br>\n",
    "Github code : <br>\n",
    "\n",
    "https://github.com/roatienza/Deep-Learning-Experiments/blob/master/Experiments/Tensorflow/RNN/rnn_words.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import random\n",
    "import collections\n",
    "import time\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "def elapsed(sec):\n",
    "    if sec<60:\n",
    "        return str(sec) + \" sec\"\n",
    "    elif sec<(60*60):\n",
    "        return str(sec/60) + \" min\"\n",
    "    else:\n",
    "        return str(sec/(60*60)) + \" hr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_path = '/tmp/tensorflow/rnn_words'\n",
    "writer = tf.summary.FileWriter(logs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file = 'belling_the_cat.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(fname):\n",
    "    with open(fname) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    content = [word for i in range(len(content)) for word in content[i].split()]\n",
    "    return np.array(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training data...\n"
     ]
    }
   ],
   "source": [
    "training_data = read_data(training_file)\n",
    "print(\"Loaded training data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary, reverse_dictionary = build_dataset(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_iters = 50000\n",
    "display_step = 1000\n",
    "n_input = 3\n",
    "\n",
    "# in RNN cell\n",
    "n_hidden = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input, 1])\n",
    "y = tf.placeholder(tf.float32, [None, vocab_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN output node weights and biases\n",
    "weights = {\n",
    "    'out' : tf.Variable(tf.random_normal([n_hidden, vocab_size]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'out' : tf.Variable(tf.random_normal([vocab_size]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(x, weights, biases):\n",
    "    \n",
    "    # reshape to [1, n_input]\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "    \n",
    "    # generate a n_input-element sequence of inputs\n",
    "    x = tf.split(x, n_input, 1)\n",
    "    \n",
    "    # 2-layer LSTM, each layer has n_hidden units.\n",
    "#     rnn_cell = rnn.multiRNNCell([rnn.BasicLSTMCell(n_hidden),\n",
    "#                                 rnn.BasicLSTMCell(n_hidden)])\n",
    "    \n",
    "    # 1 layer\n",
    "    rnn_cell = tf.nn.rnn_cell.LSTMCell(n_hidden, name='basic_lstm_cell')\n",
    "    \n",
    "    # generate prediction\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "    \n",
    "    return tf.matmul(outputs[-1], weights['out'] + biases['out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = RNN(x, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred,\n",
    "                                                                labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter= 1000, Average Loss= 6.974286, Average Accuracy= 6.50%\n",
      "['to', 'bell', 'the'] - [cat] vs [this]\n",
      "Iter= 2000, Average Loss= 3.392263, Average Accuracy= 22.20%\n",
      "['easily', 'retire', 'while'] - [she] vs [and]\n",
      "Iter= 3000, Average Loss= 3.328163, Average Accuracy= 20.10%\n",
      "['.', 'by', 'this'] - [means] vs [a]\n",
      "Iter= 4000, Average Loss= 2.702323, Average Accuracy= 27.90%\n",
      "['be', 'procured', ','] - [and] vs [then]\n",
      "Iter= 5000, Average Loss= 2.209367, Average Accuracy= 36.90%\n",
      "['therefore', ',', 'to'] - [propose] vs [could]\n",
      "Iter= 6000, Average Loss= 1.995959, Average Accuracy= 38.90%\n",
      "['enemy', 'approaches', 'us'] - [.] vs [.]\n",
      "Iter= 7000, Average Loss= 1.564586, Average Accuracy= 45.90%\n",
      "['chief', 'danger', 'consists'] - [in] vs [when]\n",
      "Iter= 8000, Average Loss= 1.438136, Average Accuracy= 51.10%\n",
      "['our', 'chief', 'danger'] - [consists] vs [,]\n",
      "Iter= 9000, Average Loss= 1.063982, Average Accuracy= 63.80%\n",
      "['would', 'meet', 'the'] - [case] vs [case]\n",
      "Iter= 10000, Average Loss= 0.953809, Average Accuracy= 67.10%\n",
      "['a', 'proposal', 'to'] - [make] vs [make]\n",
      "Iter= 11000, Average Loss= 0.897721, Average Accuracy= 71.70%\n",
      "['at', 'last', 'a'] - [young] vs [consider]\n",
      "Iter= 12000, Average Loss= 0.691593, Average Accuracy= 76.10%\n",
      "['.', 'some', 'said'] - [this] vs [this]\n",
      "Iter= 13000, Average Loss= 0.723652, Average Accuracy= 74.80%\n",
      "['had', 'a', 'general'] - [council] vs [council]\n",
      "Iter= 14000, Average Loss= 0.645237, Average Accuracy= 78.40%\n",
      "['the', 'old', 'mouse'] - [said] vs [said]\n",
      "Iter= 15000, Average Loss= 0.547615, Average Accuracy= 79.40%\n",
      "['to', 'bell', 'the'] - [cat] vs [cat]\n",
      "Iter= 16000, Average Loss= 0.583813, Average Accuracy= 78.80%\n",
      "['got', 'up', 'and'] - [said] vs [said]\n",
      "Iter= 17000, Average Loss= 0.507985, Average Accuracy= 80.80%\n",
      "['she', 'was', 'in'] - [the] vs [the]\n",
      "Iter= 18000, Average Loss= 0.539626, Average Accuracy= 81.80%\n",
      "['know', 'when', 'she'] - [was] vs [was]\n",
      "Iter= 19000, Average Loss= 0.624314, Average Accuracy= 77.00%\n",
      "['ribbon', 'round', 'the'] - [neck] vs [neck]\n",
      "Iter= 20000, Average Loss= 0.440442, Average Accuracy= 81.90%\n",
      "['small', 'bell', 'be'] - [procured] vs [procured]\n",
      "Iter= 21000, Average Loss= 0.475707, Average Accuracy= 80.20%\n",
      "['from', 'her', '.'] - [i] vs [i]\n",
      "Iter= 22000, Average Loss= 0.382972, Average Accuracy= 84.70%\n",
      "['approach', ',', 'we'] - [could] vs [could]\n",
      "Iter= 23000, Average Loss= 0.322894, Average Accuracy= 87.00%\n",
      "['.', 'now', ','] - [if] vs [if]\n",
      "Iter= 24000, Average Loss= 0.465781, Average Accuracy= 80.80%\n",
      "['manner', 'in', 'which'] - [the] vs [the]\n",
      "Iter= 25000, Average Loss= 0.301124, Average Accuracy= 84.60%\n",
      "['all', 'agree', ','] - [said] vs [said]\n",
      "Iter= 26000, Average Loss= 0.322665, Average Accuracy= 86.00%\n",
      "['he', 'thought', 'would'] - [meet] vs [meet]\n",
      "Iter= 27000, Average Loss= 0.451864, Average Accuracy= 83.40%\n",
      "['up', 'and', 'said'] - [he] vs [that]\n",
      "Iter= 28000, Average Loss= 0.295480, Average Accuracy= 88.00%\n",
      "['cat', '.', 'some'] - [said] vs [said]\n",
      "Iter= 29000, Average Loss= 0.296478, Average Accuracy= 86.50%\n",
      "['propose', 'impossible', 'remedies'] - [.] vs [.]\n",
      "Iter= 30000, Average Loss= 0.284986, Average Accuracy= 88.90%\n",
      "['the', 'mice', 'looked'] - [at] vs [at]\n",
      "Iter= 31000, Average Loss= 0.241715, Average Accuracy= 89.50%\n",
      "['said', 'that', 'is'] - [all] vs [all]\n",
      "Iter= 32000, Average Loss= 0.251503, Average Accuracy= 89.20%\n",
      "['general', 'applause', ','] - [until] vs [until]\n",
      "Iter= 33000, Average Loss= 0.329295, Average Accuracy= 87.50%\n",
      "['was', 'in', 'the'] - [neighbourhood] vs [neighbourhood]\n",
      "Iter= 34000, Average Loss= 0.332827, Average Accuracy= 85.00%\n",
      "['.', 'by', 'this'] - [means] vs [means]\n",
      "Iter= 35000, Average Loss= 0.256214, Average Accuracy= 88.70%\n",
      "['easily', 'escape', 'from'] - [her] vs [her]\n",
      "Iter= 36000, Average Loss= 0.183508, Average Accuracy= 90.30%\n",
      "['.', 'now', ','] - [if] vs [if]\n",
      "Iter= 37000, Average Loss= 0.321451, Average Accuracy= 86.60%\n",
      "['chief', 'danger', 'consists'] - [in] vs [in]\n",
      "Iter= 38000, Average Loss= 0.234228, Average Accuracy= 89.50%\n",
      "['had', 'a', 'proposal'] - [to] vs [to]\n",
      "Iter= 39000, Average Loss= 0.225202, Average Accuracy= 88.70%\n",
      "['.', 'some', 'said'] - [this] vs [this]\n",
      "Iter= 40000, Average Loss= 0.165232, Average Accuracy= 91.80%\n",
      "['mice', 'had', 'a'] - [general] vs [general]\n",
      "Iter= 41000, Average Loss= 0.231884, Average Accuracy= 88.80%\n",
      "['bell', 'the', 'cat'] - [?] vs [?]\n",
      "Iter= 42000, Average Loss= 0.296090, Average Accuracy= 88.20%\n",
      "['an', 'old', 'mouse'] - [got] vs [got]\n",
      "Iter= 43000, Average Loss= 0.225602, Average Accuracy= 89.50%\n",
      "['retire', 'while', 'she'] - [was] vs [was]\n",
      "Iter= 44000, Average Loss= 0.206422, Average Accuracy= 90.90%\n",
      "['neck', 'of', 'the'] - [cat] vs [cat]\n",
      "Iter= 45000, Average Loss= 0.278419, Average Accuracy= 88.10%\n",
      "['i', 'venture', ','] - [therefore] vs [therefore]\n",
      "Iter= 46000, Average Loss= 0.218273, Average Accuracy= 89.10%\n",
      "['receive', 'some', 'signal'] - [of] vs [of]\n",
      "Iter= 47000, Average Loss= 0.185123, Average Accuracy= 91.00%\n",
      "['consists', 'in', 'the'] - [sly] vs [sly]\n",
      "Iter= 48000, Average Loss= 0.195625, Average Accuracy= 90.50%\n",
      "['case', '.', 'you'] - [will] vs [will]\n",
      "Iter= 49000, Average Loss= 0.285907, Average Accuracy= 87.90%\n",
      "['a', 'young', 'mouse'] - [got] vs [got]\n",
      "Iter= 50000, Average Loss= 0.272102, Average Accuracy= 88.60%\n",
      "['this', ',', 'and'] - [some] vs [some]\n",
      "Optimization Finished!\n",
      "Elapsed time:  9.372049156824747 min\n",
      "Run on command line.\n",
      "\ttensorboard --logdir=/tmp/tensorflow/rnn_words\n",
      "Point your web browser to: http://localhost:6006/\n",
      "3 words: this this this\n",
      "this this this , but our thought . case will all agree , said he , that our chief danger in the sly and treacherous manner in the sly and treacherous manner in the sly\n",
      "3 words: manner in the\n",
      "manner in the sly and treacherous manner in the sly and treacherous manner in the sly and treacherous manner in the sly and treacherous manner in the sly and treacherous manner in the sly and\n",
      "3 words: this thought thought\n",
      "this thought thought meet the case . you will all agree , said he , that our chief danger in the sly and treacherous manner in the sly and treacherous manner in the sly and\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 0\n",
    "    offset = random.randint(0, n_input+1)\n",
    "    end_offset = n_input+1\n",
    "    acc_total = 0\n",
    "    loss_total = 0\n",
    "    \n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    while step < training_iters:\n",
    "        if offset > (len(training_data)-end_offset):\n",
    "            offset = random.randint(0, n_input+1)\n",
    "            \n",
    "        symbols_in_keys = [ [dictionary[ str(training_data[i])]] for i in range(offset, offset+n_input)]\n",
    "        symbols_in_keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
    "        \n",
    "        symbols_out_onehot = np.zeros([vocab_size], dtype=float)\n",
    "        symbols_out_onehot[dictionary[str(training_data[offset+n_input])]] = 1.0\n",
    "        symbols_out_onehot = np.reshape(symbols_out_onehot, [1, -1])\n",
    "        \n",
    "        _, acc, loss, onehot_pred = sess.run([optimizer, accuracy, cost, pred], \\\n",
    "                                             feed_dict={x: symbols_in_keys, y:symbols_out_onehot})\n",
    "        \n",
    "        loss_total += loss\n",
    "        acc_total += acc\n",
    "        \n",
    "        if (step+1) % display_step == 0:\n",
    "            print(\"Iter= \" + str(step+1) + \", Average Loss= \" + \\\n",
    "                 \"{:.6f}\".format(loss_total/display_step) + \", Average Accuracy= \" + \\\n",
    "                 \"{:.2f}%\".format(100*acc_total/display_step))\n",
    "            \n",
    "            acc_total = 0\n",
    "            loss_total = 0\n",
    "            symbols_in = [training_data[i] for i in range(offset, offset + n_input)]\n",
    "            symbols_out = training_data[offset + n_input]\n",
    "            symbols_out_pred = reverse_dictionary[ int(tf.argmax(onehot_pred, 1).eval())]\n",
    "            print(\"%s - [%s] vs [%s]\" % (symbols_in, symbols_out, symbols_out_pred))\n",
    "        step += 1\n",
    "        offset += (n_input+1)\n",
    "    \n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Elapsed time: \", elapsed(time.time() - start_time))\n",
    "    print(\"Run on command line.\")\n",
    "    \n",
    "    print(\"\\ttensorboard --logdir=%s\" % (logs_path))\n",
    "    print(\"Point your web browser to: http://localhost:6006/\")\n",
    "    \n",
    "    while True:\n",
    "        prompt = \"%s words: \" % n_input\n",
    "        sentence = input(prompt)\n",
    "        sentence = sentence.strip()\n",
    "        words = sentence.split(' ')\n",
    "        \n",
    "        if len(words) != n_input:\n",
    "            continue\n",
    "        try:\n",
    "            symbols_in_keys = [dictionary[str(words[i])] for i in range(len(words))]\n",
    "            for i in range(32):\n",
    "                keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
    "                onehot_pred = sess.run(pred, feed_dict={x:keys})\n",
    "                onehot_pred_index = int(tf.argmax(onehot_pred,1).eval())\n",
    "                sentence = \"%s %s\" % (sentence, reverse_dictionary[onehot_pred_index])\n",
    "                symbols_in_keys = symbols_in_keys[1:]\n",
    "                symbols_in_keys.append(onehot_pred_index)\n",
    "            print(sentence)\n",
    "        except:\n",
    "            print(\"Word not in dictionary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
