{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import set_random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 100\n",
    "N_HIDDEN_HL1 = 10\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "seed(RANDOM_STATE)\n",
    "set_random_seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_pickle(path):\n",
    "    import pickle\n",
    "    with open(path, 'rb') as f:\n",
    "        X = pickle.load(f)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_original = open_pickle('../../data/imdb/imdb_original_preprocessed_xtrain.pickle')\n",
    "X_test_original = open_pickle('../../data/imdb/imdb_original_preprocessed_xtest.pickle')\n",
    "y_train_original = open_pickle('../../data/imdb/imdb_original_preprocessed_ytrain.pickle')\n",
    "y_test_original = open_pickle('../../data/imdb/imdb_original_preprocessed_ytest.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "token = r\"(?u)\\b[\\w\\'/]+\\b\"\n",
    "cv = CountVectorizer(min_df = 100, token_pattern=token, lowercase=True, binary=True)\n",
    "X_train = cv.fit_transform(X_train_original)\n",
    "X_test = cv.transform(X_test_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = np.expand_dims(X_train, axis=0)\n",
    "X_te = np.expand_dims(X_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3686)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = np.reshape(y_train_original, (len(y_train_original), 1))\n",
    "y_te = np.reshape(y_test_original, (len(y_test_original), 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start here\n",
    "\n",
    "minimizing -cost is the same as maximizing cost <br>\n",
    "    \n",
    "https://github.com/Mazecreator/tensorflow-hints/tree/master/maximize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 out of 100 loss: 0.83209014\n",
      "Epoch 1 out of 100 loss: 0.73382485\n",
      "Epoch 2 out of 100 loss: 0.7439606\n",
      "Epoch 3 out of 100 loss: 0.6909256\n",
      "Epoch 4 out of 100 loss: 0.6253298\n",
      "Epoch 5 out of 100 loss: 0.5939575\n",
      "Epoch 6 out of 100 loss: 0.58605987\n",
      "Epoch 7 out of 100 loss: 0.5667689\n",
      "Epoch 8 out of 100 loss: 0.53330064\n",
      "Epoch 9 out of 100 loss: 0.5025066\n",
      "Epoch 10 out of 100 loss: 0.4852207\n",
      "Epoch 11 out of 100 loss: 0.4773257\n",
      "Epoch 12 out of 100 loss: 0.46836397\n",
      "Epoch 13 out of 100 loss: 0.45361507\n",
      "Epoch 14 out of 100 loss: 0.43621057\n",
      "Epoch 15 out of 100 loss: 0.4216453\n",
      "Epoch 16 out of 100 loss: 0.41233855\n",
      "Epoch 17 out of 100 loss: 0.4063517\n",
      "Epoch 18 out of 100 loss: 0.40006304\n",
      "Epoch 19 out of 100 loss: 0.39152196\n",
      "Epoch 20 out of 100 loss: 0.38148984\n",
      "Epoch 21 out of 100 loss: 0.37208802\n",
      "Epoch 22 out of 100 loss: 0.3648034\n",
      "Epoch 23 out of 100 loss: 0.3594811\n",
      "Epoch 24 out of 100 loss: 0.35479406\n",
      "Epoch 25 out of 100 loss: 0.34948942\n",
      "Epoch 26 out of 100 loss: 0.3433046\n",
      "Epoch 27 out of 100 loss: 0.33692867\n",
      "Epoch 28 out of 100 loss: 0.33127314\n",
      "Epoch 29 out of 100 loss: 0.32674474\n",
      "Epoch 30 out of 100 loss: 0.32303977\n",
      "Epoch 31 out of 100 loss: 0.31949782\n",
      "Epoch 32 out of 100 loss: 0.31564948\n",
      "Epoch 33 out of 100 loss: 0.31151348\n",
      "Epoch 34 out of 100 loss: 0.30747584\n",
      "Epoch 35 out of 100 loss: 0.30392265\n",
      "Epoch 36 out of 100 loss: 0.30094987\n",
      "Epoch 37 out of 100 loss: 0.2983341\n",
      "Epoch 38 out of 100 loss: 0.295748\n",
      "Epoch 39 out of 100 loss: 0.29301286\n",
      "Epoch 40 out of 100 loss: 0.29019243\n",
      "Epoch 41 out of 100 loss: 0.28749374\n",
      "Epoch 42 out of 100 loss: 0.2850775\n",
      "Epoch 43 out of 100 loss: 0.2829406\n",
      "Epoch 44 out of 100 loss: 0.28094366\n",
      "Epoch 45 out of 100 loss: 0.27893203\n",
      "Epoch 46 out of 100 loss: 0.27685094\n",
      "Epoch 47 out of 100 loss: 0.27476284\n",
      "Epoch 48 out of 100 loss: 0.27277586\n",
      "Epoch 49 out of 100 loss: 0.27094916\n",
      "Epoch 50 out of 100 loss: 0.2692568\n",
      "Epoch 51 out of 100 loss: 0.26761824\n",
      "Epoch 52 out of 100 loss: 0.26596874\n",
      "Epoch 53 out of 100 loss: 0.26430207\n",
      "Epoch 54 out of 100 loss: 0.2626643\n",
      "Epoch 55 out of 100 loss: 0.26110598\n",
      "Epoch 56 out of 100 loss: 0.2596428\n",
      "Epoch 57 out of 100 loss: 0.25824758\n",
      "Epoch 58 out of 100 loss: 0.25687778\n",
      "Epoch 59 out of 100 loss: 0.25551015\n",
      "Epoch 60 out of 100 loss: 0.25415298\n",
      "Epoch 61 out of 100 loss: 0.2528335\n",
      "Epoch 62 out of 100 loss: 0.25157106\n",
      "Epoch 63 out of 100 loss: 0.2503636\n",
      "Epoch 64 out of 100 loss: 0.24919122\n",
      "Epoch 65 out of 100 loss: 0.24803379\n",
      "Epoch 66 out of 100 loss: 0.24688615\n",
      "Epoch 67 out of 100 loss: 0.24575694\n",
      "Epoch 68 out of 100 loss: 0.2446602\n",
      "Epoch 69 out of 100 loss: 0.24360082\n",
      "Epoch 70 out of 100 loss: 0.24257258\n",
      "Epoch 71 out of 100 loss: 0.24156344\n",
      "Epoch 72 out of 100 loss: 0.24056546\n",
      "Epoch 73 out of 100 loss: 0.2395808\n",
      "Epoch 74 out of 100 loss: 0.23861551\n",
      "Epoch 75 out of 100 loss: 0.23767479\n",
      "Epoch 76 out of 100 loss: 0.2367582\n",
      "Epoch 77 out of 100 loss: 0.23586047\n",
      "Epoch 78 out of 100 loss: 0.23497605\n",
      "Epoch 79 out of 100 loss: 0.23410305\n",
      "Epoch 80 out of 100 loss: 0.23324423\n",
      "Epoch 81 out of 100 loss: 0.23240246\n",
      "Epoch 82 out of 100 loss: 0.2315794\n",
      "Epoch 83 out of 100 loss: 0.2307729\n",
      "Epoch 84 out of 100 loss: 0.22997975\n",
      "Epoch 85 out of 100 loss: 0.2291977\n",
      "Epoch 86 out of 100 loss: 0.22842719\n",
      "Epoch 87 out of 100 loss: 0.22767\n",
      "Epoch 88 out of 100 loss: 0.22692703\n",
      "Epoch 89 out of 100 loss: 0.22619809\n",
      "Epoch 90 out of 100 loss: 0.22548088\n",
      "Epoch 91 out of 100 loss: 0.22477436\n",
      "Epoch 92 out of 100 loss: 0.22407793\n",
      "Epoch 93 out of 100 loss: 0.22339177\n",
      "Epoch 94 out of 100 loss: 0.22271718\n",
      "Epoch 95 out of 100 loss: 0.22205368\n",
      "Epoch 96 out of 100 loss: 0.22140102\n",
      "Epoch 97 out of 100 loss: 0.2207573\n",
      "Epoch 98 out of 100 loss: 0.2201225\n",
      "Epoch 99 out of 100 loss: 0.21949689\n",
      "Accuracy 0.8773999810218811\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X_train_tensor = tf.placeholder(tf.float32, [None, X_train.shape[1]], name='review')\n",
    "Y_train_tensor = tf.placeholder(tf.float32, [None, 1], name='label')\n",
    "\n",
    "W = tf.get_variable(name='weights',\n",
    "                   shape=(X_train.shape[1], 1), \n",
    "                   initializer=tf.glorot_uniform_initializer(seed=RANDOM_STATE))\n",
    "\n",
    "b = tf.get_variable(name='bias', \n",
    "                   initializer=tf.constant(1.0))\n",
    "\n",
    "# Final output logits\n",
    "logits = tf.matmul(X_train_tensor, W) + b\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,\n",
    "                                                             labels=Y_train_tensor))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "preds = tf.nn.sigmoid(logits)\n",
    "correct_preds = tf.equal(tf.cast(tf.greater(preds, tf.constant(0.5)), tf.float32), \n",
    "                         Y_train_tensor)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_preds, tf.float32))\n",
    "\n",
    "writer = tf.summary.FileWriter('./graphs/imdb_simple', tf.get_default_graph())\n",
    "\n",
    "# Start the session\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "        \n",
    "        _, loss_per_epoch = sess.run([optimizer, loss], \n",
    "                           feed_dict={X_train_tensor: X_train, Y_train_tensor: y_tr})\n",
    "        \n",
    "        print('Epoch', epoch, 'out of', EPOCHS, 'loss:', loss_per_epoch)\n",
    "    \n",
    "    accuracy_test = sess.run(accuracy, \n",
    "                             feed_dict={X_train_tensor: X_test, Y_train_tensor: y_te})\n",
    "    \n",
    "    print('Accuracy {0}'.format(accuracy_test))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human term IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_unigrams(path, X, y):\n",
    "    word_list = []\n",
    "    connotation = {}\n",
    "    \n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            word_list.append(line.strip())\n",
    "            \n",
    "    for word in word_list:\n",
    "        pos_count = 0\n",
    "        neg_count = 0\n",
    "        for i, doc in enumerate(X):\n",
    "            if word in doc.lower():\n",
    "                \n",
    "                if (y[i] == 1):\n",
    "                    pos_count += 1\n",
    "                else:\n",
    "                    neg_count += 1\n",
    "                    \n",
    "        if pos_count > neg_count:\n",
    "            connotation[word] = 1\n",
    "        else:\n",
    "            connotation[word] = 0\n",
    "    \n",
    "    return word_list, connotation\n",
    "\n",
    "def generate_appearance(X_train_corpus, X_test_corpus, word_list, connotation):\n",
    "    y_train_agreement = []\n",
    "    for i in range(len(X_train_corpus)):\n",
    "        doc_agreement = []\n",
    "        for word in word_list:\n",
    "            if word in X_train_corpus[i]:\n",
    "                if connotation[word] == 1:\n",
    "                    doc_agreement.append(1)\n",
    "                else:\n",
    "                    doc_agreement.append(-1)\n",
    "            else:\n",
    "                doc_agreement.append(0)\n",
    "        y_train_agreement.append(doc_agreement)\n",
    "        \n",
    "    y_test_agreement = []\n",
    "    for i in range(len(X_test_corpus)):\n",
    "        doc_agreement = []\n",
    "        for word in word_list:\n",
    "            if word in X_test_corpus[i]:\n",
    "                if connotation[word] == 1:\n",
    "                    doc_agreement.append(1)\n",
    "                else:\n",
    "                    doc_agreement.append(-1)\n",
    "            else:\n",
    "                doc_agreement.append(0)\n",
    "        y_test_agreement.append(doc_agreement)\n",
    "        \n",
    "    return np.array(y_train_agreement), np.array(y_test_agreement)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list, connotation = load_unigrams('./imdb-unigrams.txt', X_train_original, y_train_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, X_train.shape[1]], name='review')\n",
    "Y = tf.placeholder(tf.float32, [None, 1], name='label')\n",
    "\n",
    "W = tf.get_variable(name='weights',\n",
    "                   shape=(X_train.shape[1], 1), \n",
    "                   initializer=tf.initializers.glorot_uniform(seed=RANDOM_STATE))\n",
    "\n",
    "b = tf.get_variable(name='bias', \n",
    "                   initializer=tf.constant(1.0))\n",
    "\n",
    "# Final output logits\n",
    "relu_op = tf.nn.relu(X)\n",
    "logits = tf.matmul(relu_op, W) + b\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits,\n",
    "                                                             labels=Y))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "preds = tf.nn.sigmoid(logits)\n",
    "correct_preds = tf.equal(tf.cast(tf.greater(preds, tf.constant(0.5)), tf.float32), \n",
    "                         Y)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_preds, tf.float32))\n",
    "\n",
    "writer = tf.summary.FileWriter('./graphs/imdb_simple', tf.get_default_graph())\n",
    "\n",
    "# Start the session\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0\n",
    "        \n",
    "        _, loss_per_epoch = sess.run([optimizer, loss], \n",
    "                           feed_dict={X: X_train, Y: y_tr})\n",
    "        \n",
    "        print('Epoch', epoch, 'out of', EPOCHS, 'loss:', loss_per_epoch)\n",
    "    \n",
    "    accuracy_test = sess.run(accuracy, \n",
    "                             feed_dict={X: X_test, Y: y_te})\n",
    "    \n",
    "    print('Accuracy {0}'.format(accuracy_test))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
